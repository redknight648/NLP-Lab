{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca97997c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from python-docx) (4.5.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from python-docx) (4.9.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1060abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import docx\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea4a287f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>documet.docx</td>\n",
       "      <td>Doc  version for the first experiment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dot.pdf</td>\n",
       "      <td>This is the pdf version for the first experime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nlp.txt</td>\n",
       "      <td>Text Pre-porcessing,lemmatization,Stemming\\nTo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rand.txt</td>\n",
       "      <td>7th semester Natural Language Processing\\nArti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       filename                                            content\n",
       "0  documet.docx              Doc  version for the first experiment\n",
       "1       dot.pdf  This is the pdf version for the first experime...\n",
       "2       nlp.txt  Text Pre-porcessing,lemmatization,Stemming\\nTo...\n",
       "3      rand.txt  7th semester Natural Language Processing\\nArti..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Program 1 – Study of Python and basic commands to access text data\n",
    "dir_path='C:\\\\Users\\\\Pooja\\\\Downloads\\\\7th sem\\\\NLP LAB'\n",
    "files=[f for f in os.listdir(dir_path) if (f.endswith('.docx') or f.endswith('.txt') or f.endswith('.pdf'))]\n",
    "\n",
    "data=[]\n",
    "for txtfile in files:\n",
    "    if txtfile.endswith('.txt'):\n",
    "        with open(os.path.join(dir_path,txtfile),'r') as file:\n",
    "            content=file.read()\n",
    "            data.append({'filename':txtfile,'content':content})\n",
    "    \n",
    "    if txtfile.endswith('.docx'):\n",
    "            doc_path=os.path.join(dir_path,txtfile)\n",
    "            doc=docx.Document(doc_path)\n",
    "            content='\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "            data.append({'filename':txtfile,'content':content})\n",
    "    \n",
    "    if txtfile.endswith('.pdf'):\n",
    "        with open(os.path.join(dir_path,txtfile),'rb') as f:\n",
    "            pdf_reader=PyPDF2.PdfReader(f)\n",
    "            num_pages=len(pdf_reader.pages)\n",
    "            for page in range(num_pages):\n",
    "                content=pdf_reader.pages[page].extract_text()\n",
    "                data.append({'filename':txtfile,'content':content})\n",
    "res=pd.DataFrame(data)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae336fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Pooja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Pooja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33033cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46755f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>content</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>documet.docx</td>\n",
       "      <td>Doc  version for the first experiment</td>\n",
       "      <td>doc version first experi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dot.pdf</td>\n",
       "      <td>This is the pdf version for the first experime...</td>\n",
       "      <td>pdf version first experi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nlp.txt</td>\n",
       "      <td>Text Pre-porcessing,lemmatization,Stemming\\nTo...</td>\n",
       "      <td>text preporcessinglemmatizationstem tokenizati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rand.txt</td>\n",
       "      <td>7th semester Natural Language Processing\\nArti...</td>\n",
       "      <td>th semest natur languag process artifici intel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       filename                                            content   \n",
       "0  documet.docx              Doc  version for the first experiment  \\\n",
       "1       dot.pdf  This is the pdf version for the first experime...   \n",
       "2       nlp.txt  Text Pre-porcessing,lemmatization,Stemming\\nTo...   \n",
       "3      rand.txt  7th semester Natural Language Processing\\nArti...   \n",
       "\n",
       "                                               clean  \n",
       "0                           doc version first experi  \n",
       "1                           pdf version first experi  \n",
       "2  text preporcessinglemmatizationstem tokenizati...  \n",
       "3  th semest natur languag process artifici intel...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Program 2 – Perform Pre-Proessing\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "ps=PorterStemmer()\n",
    "stopwords=set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text=re.sub(r'[^A-Za-z\\s]','',text)\n",
    "    text=text.lower()\n",
    "    tokens=word_tokenize(text)\n",
    "    tokens=[ps.stem(text) for text in tokens if text not in stopwords]\n",
    "    \n",
    "    return ' '.join(tokens) #j2\n",
    "\n",
    "df['clean']=df['content'].apply(clean_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7529143a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rand.txt</td>\n",
       "      <td>7th semester lab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file.txt</td>\n",
       "      <td>natural language processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one.txt</td>\n",
       "      <td>i like reading books</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   filename                      content\n",
       "0  rand.txt             7th semester lab\n",
       "1  file.txt  natural language processing\n",
       "2   one.txt         i like reading books"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=[['rand.txt','7th semester lab'],['file.txt','natural language processing'],['one.txt','i like reading books']]\n",
    "df2=pd.DataFrame(data,columns=['filename','content'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f20ac3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>content</th>\n",
       "      <th>clean</th>\n",
       "      <th>bigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>documet.docx</td>\n",
       "      <td>Doc  version for the first experiment</td>\n",
       "      <td>doc version first experi</td>\n",
       "      <td>[doc version, version first, first experi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dot.pdf</td>\n",
       "      <td>This is the pdf version for the first experime...</td>\n",
       "      <td>pdf version first experi</td>\n",
       "      <td>[pdf version, version first, first experi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nlp.txt</td>\n",
       "      <td>Text Pre-porcessing,lemmatization,Stemming\\nTo...</td>\n",
       "      <td>text preporcessinglemmatizationstem tokenizati...</td>\n",
       "      <td>[text preporcessinglemmatizationstem, preporce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rand.txt</td>\n",
       "      <td>7th semester Natural Language Processing\\nArti...</td>\n",
       "      <td>th semest natur languag process artifici intel...</td>\n",
       "      <td>[th semest, semest natur, natur languag, langu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       filename                                            content   \n",
       "0  documet.docx              Doc  version for the first experiment  \\\n",
       "1       dot.pdf  This is the pdf version for the first experime...   \n",
       "2       nlp.txt  Text Pre-porcessing,lemmatization,Stemming\\nTo...   \n",
       "3      rand.txt  7th semester Natural Language Processing\\nArti...   \n",
       "\n",
       "                                               clean   \n",
       "0                           doc version first experi  \\\n",
       "1                           pdf version first experi   \n",
       "2  text preporcessinglemmatizationstem tokenizati...   \n",
       "3  th semest natur languag process artifici intel...   \n",
       "\n",
       "                                              bigram  \n",
       "0         [doc version, version first, first experi]  \n",
       "1         [pdf version, version first, first experi]  \n",
       "2  [text preporcessinglemmatizationstem, preporce...  \n",
       "3  [th semest, semest natur, natur languag, langu...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Program 3 – Implement N-Gram Model\n",
    "from nltk.util import ngrams\n",
    "  #LIST1\n",
    "def gen_ngram(text,n):\n",
    "    text=text.split()\n",
    "    return [' '.join(gram) for gram in ngrams(text,n)] #j3\n",
    "\n",
    "df['bigram']=df['clean'].apply(gen_ngram,n=2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7d1f987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>content</th>\n",
       "      <th>bigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rand.txt</td>\n",
       "      <td>7th semester lab</td>\n",
       "      <td>[7th, semester, lab]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file.txt</td>\n",
       "      <td>natural language processing</td>\n",
       "      <td>[natural, language, processing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one.txt</td>\n",
       "      <td>i like reading books</td>\n",
       "      <td>[i, like, reading, books]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   filename                      content                           bigram\n",
       "0  rand.txt             7th semester lab             [7th, semester, lab]\n",
       "1  file.txt  natural language processing  [natural, language, processing]\n",
       "2   one.txt         i like reading books        [i, like, reading, books]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['bigram']=df2['content'].apply(gen_ngram,n=1)\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "619911a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Pooja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>content</th>\n",
       "      <th>bigram</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rand.txt</td>\n",
       "      <td>7th semester lab</td>\n",
       "      <td>[7th, semester, lab]</td>\n",
       "      <td>[(7th, CD), (semester, NN), (lab, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file.txt</td>\n",
       "      <td>natural language processing</td>\n",
       "      <td>[natural, language, processing]</td>\n",
       "      <td>[(natural, JJ), (language, NN), (processing, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one.txt</td>\n",
       "      <td>i like reading books</td>\n",
       "      <td>[i, like, reading, books]</td>\n",
       "      <td>[(i, NNS), (like, VBP), (reading, VBG), (books...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   filename                      content                           bigram   \n",
       "0  rand.txt             7th semester lab             [7th, semester, lab]  \\\n",
       "1  file.txt  natural language processing  [natural, language, processing]   \n",
       "2   one.txt         i like reading books        [i, like, reading, books]   \n",
       "\n",
       "                                                 pos  \n",
       "0             [(7th, CD), (semester, NN), (lab, NN)]  \n",
       "1  [(natural, JJ), (language, NN), (processing, NN)]  \n",
       "2  [(i, NNS), (like, VBP), (reading, VBG), (books...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Program 4 – Implement POS Tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def genpos_tag(text):\n",
    "    token=word_tokenize(text)\n",
    "    return pos_tag(token)\n",
    "\n",
    "df2['pos']=df2['content'].apply(genpos_tag)\n",
    "df2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909243a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a476363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading maxent_ne_chuker: Package 'maxent_ne_chuker'\n",
      "[nltk_data]     not found in index\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Pooja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>content</th>\n",
       "      <th>bigram</th>\n",
       "      <th>pos</th>\n",
       "      <th>chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rand.txt</td>\n",
       "      <td>7th semester lab</td>\n",
       "      <td>[7th, semester, lab]</td>\n",
       "      <td>[(7th, CD), (semester, NN), (lab, NN)]</td>\n",
       "      <td>[semester, lab]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file.txt</td>\n",
       "      <td>natural language processing</td>\n",
       "      <td>[natural, language, processing]</td>\n",
       "      <td>[(natural, JJ), (language, NN), (processing, NN)]</td>\n",
       "      <td>[natural language, processing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one.txt</td>\n",
       "      <td>i like reading books</td>\n",
       "      <td>[i, like, reading, books]</td>\n",
       "      <td>[(i, NNS), (like, VBP), (reading, VBG), (books...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   filename                      content                           bigram   \n",
       "0  rand.txt             7th semester lab             [7th, semester, lab]  \\\n",
       "1  file.txt  natural language processing  [natural, language, processing]   \n",
       "2   one.txt         i like reading books        [i, like, reading, books]   \n",
       "\n",
       "                                                 pos   \n",
       "0             [(7th, CD), (semester, NN), (lab, NN)]  \\\n",
       "1  [(natural, JJ), (language, NN), (processing, NN)]   \n",
       "2  [(i, NNS), (like, VBP), (reading, VBG), (books...   \n",
       "\n",
       "                            chunk  \n",
       "0                 [semester, lab]  \n",
       "1  [natural language, processing]  \n",
       "2                              []  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Program 5 – Implement Chunking\n",
    "nltk.download('maxent_ne_chuker')   \n",
    "nltk.download('words')\n",
    "import nltk\n",
    "\n",
    "def genchunk(text):\n",
    "    grammar=\"NP: {<DT>?<JJ>*<NN>}\"\n",
    "    cp=nltk.RegexpParser(grammar)\n",
    "    tree=cp.parse(text)  #parse\n",
    "    \n",
    "    NP=[]\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == \"NP\":\n",
    "            NP.append(' '.join(word for word, tag in subtree.leaves())) #j5\n",
    "\n",
    "    return NP\n",
    "\n",
    "df2['chunk']=df2['pos'].apply(genchunk) #apply on pos\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f34df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff=df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "04bcd7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possible completiosn\n",
      "- afer a long day at work,i like to relax by reading a book\n",
      "- afer a long day at work,i like to relax by watching tv\n",
      "- afer a long day at work,i like to relax by going for walk\n"
     ]
    }
   ],
   "source": [
    "#Program 6 – Implement Sentence Completion\n",
    "import random   #LIST2\n",
    "sentence_prompts={'she opened the door and saw a':['bright light','mysterious figure','beautiful garden'],\n",
    "                 'afer a long day at work,i like to relax by':['reading a book','watching tv','going for walk']}\n",
    "input_pr='afer a long day at work,i like to relax by'\n",
    "\n",
    "if input_pr in sentence_prompts:\n",
    "    possib=sentence_prompts[input_pr]\n",
    "    \n",
    "    print('possible completiosn')\n",
    "    for comp in possib:\n",
    "        print(f'- {input_pr} {comp}')\n",
    "else:\n",
    "    print('not in dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f81ed6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Using cached textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk>=3.1->textblob) (1.3.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Requirement already satisfied: click in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk>=3.1->textblob) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk>=3.1->textblob) (2023.5.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c0428b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>sentim</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i love this product!</td>\n",
       "      <td>0.625</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is terrible</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>neagative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral sentiment</td>\n",
       "      <td>0.000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   data  sentim      label\n",
       "0  i love this product!   0.625   positive\n",
       "1      this is terrible  -1.000  neagative\n",
       "2     neutral sentiment   0.000    neutral"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Program 7 - Sentiment Analysis\n",
    "from textblob import TextBlob\n",
    "data=['i love this product!','this is terrible','neutral sentiment']\n",
    "sentiment=[TextBlob(t).sentiment.polarity for t in data]\n",
    "label=['positive' if score>0 else 'neagative' if score<0 else 'neutral' for score in sentiment]\n",
    "d=pd.DataFrame({'data':data,'sentim':sentiment,'label':label})\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8183d492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-extractive-summarizer in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from bert-extractive-summarizer) (1.3.0)\n",
      "Requirement already satisfied: spacy in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from bert-extractive-summarizer) (3.7.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from bert-extractive-summarizer) (4.32.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from scikit-learn->bert-extractive-summarizer) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from scikit-learn->bert-extractive-summarizer) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from scikit-learn->bert-extractive-summarizer) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from scikit-learn->bert-extractive-summarizer) (1.11.2)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy->bert-extractive-summarizer) (8.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy->bert-extractive-summarizer) (4.65.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy->bert-extractive-summarizer) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy->bert-extractive-summarizer) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy->bert-extractive-summarizer) (1.10.7)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy->bert-extractive-summarizer) (1.1.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy->bert-extractive-summarizer) (3.1.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy->bert-extractive-summarizer) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy->bert-extractive-summarizer) (2.4.8)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy->bert-extractive-summarizer) (0.3.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy->bert-extractive-summarizer) (65.5.1)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy->bert-extractive-summarizer) (0.7.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy->bert-extractive-summarizer) (6.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy->bert-extractive-summarizer) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy->bert-extractive-summarizer) (23.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy->bert-extractive-summarizer) (1.0.10)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy->bert-extractive-summarizer) (2.0.10)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy->bert-extractive-summarizer) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers->bert-extractive-summarizer) (2023.5.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers->bert-extractive-summarizer) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers->bert-extractive-summarizer) (0.3.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers->bert-extractive-summarizer) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers->bert-extractive-summarizer) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers->bert-extractive-summarizer) (0.16.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers->bert-extractive-summarizer) (4.5.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers->bert-extractive-summarizer) (2023.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.1.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy->bert-extractive-summarizer) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy->bert-extractive-summarizer) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy->bert-extractive-summarizer) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy->bert-extractive-summarizer) (8.1.3)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy->bert-extractive-summarizer) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from jinja2->spacy->bert-extractive-summarizer) (2.1.2)\n",
      "Requirement already satisfied: summarizer in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (0.0.7)\n",
      "Requirement already satisfied: nltk in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from summarizer) (3.8.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk->summarizer) (1.3.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk->summarizer) (4.65.0)\n",
      "Requirement already satisfied: click in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk->summarizer) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk->summarizer) (2023.5.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from click->nltk->summarizer) (0.4.6)\n",
      "Requirement already satisfied: transformers in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: requests in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pooja\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->transformers) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-extractive-summarizer\n",
    "!pip install summarizer\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1c82424f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pooja\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:217: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[21603,    10,   622,  6577,    13, 21299,    19,     8,  1251,   484,\n",
      "           150,  1109,   920,    21,     8,  1771,   648, 31666,     6,   532,\n",
      "           813,  2408,   152,   343,  9066,   159,     3,     9,  7690,     3,\n",
      "            17,    15,   107,    75,  7742,   152,    44,  3671,    86,  2248,\n",
      "            17,    76,    15,    13, 21299,    16,     8,  1480,  8754,    31,\n",
      "             7,   116,  7285, 14387,   341, 23990,    15,    26,     5,   634,\n",
      "           733,  3511,    57,  6714,    53,   149,  9066,   141,  5153,   160,\n",
      "          2325,     7,  1952,     3,  3565,     8, 23463,     7,    16,   280,\n",
      "            11,  1550,    30,    12,   522,   186,    72, 14637,     6,  6279,\n",
      "            19,   116,   255,  6326,     7,   707,     5, 27280,     6,     9,\n",
      "           168,     3,  9623, 17901,    28,   633, 10142,    11, 30022,     7,\n",
      "            12,   112,   564,     6, 19712,   192, 13037,     7,   989,    95,\n",
      "            30,     3,     9,  2027,    13,  1044,    18, 19315,  1890,    63,\n",
      "            11, 22496,     5,   196,    17,    31,     7,   865,  1587,     8,\n",
      "           414,    62,   217,   149,  9066,  2870,     7,    21,   887,    31,\n",
      "             7,  2166,   232,    19,  5241,    12,  1175,    52,   160,  1102,\n",
      "            38,     3,     9,     3,  6482,   343,   788,    12,    73,  1161,\n",
      "             7,    15,    35,  4616,    11,  5542,    95,  6109,     3,     9,\n",
      "          4390,  4245,    38,     3,     9,  3989,  2290,   113, 18198,     7,\n",
      "           633,   887,    66,   300,     8,   296,    12,  6665,    70,  6612,\n",
      "            57,  2119,   135,     3, 11366,    80,     3, 23098,   144,     3,\n",
      "             9,    97,     1]])\n",
      "tensor([[    0,   622,  6577,    13, 21299,    19,     8,  1251,   484,   150,\n",
      "          1109,   920,    21,     8,  1771,   648, 31666,     3,     5,     8,\n",
      "           813,  2408,   152,   343,  9066,    19,     3,     9,  7690,     3,\n",
      "            17,    15,   107,    75,  7742,   152,    44, 11268,    16,  2248,\n",
      "            17,    76,    15,    13,     3, 11366,    16,     8,  1480,  8754,\n",
      "            31,     7,     3,     5,     1]])\n",
      "<pad> Lessons of Chemistry is the latest book nominated for the Sunday times bestseller. the protaganist Elizabeth is a lab tehcnician at royal institue of chemistry in the late 1960's.</s>\n",
      "Lessons of Chemistry is the latest book nominated for the Sunday times bestseller. the protaganist Elizabeth is a lab tehcnician at royal institue of chemistry in the late 1960's.\n"
     ]
    }
   ],
   "source": [
    "#Program 8 - Text Summarisation (Abstractive and Extractive)\n",
    " \n",
    "a1=\"Lessons of Chemistry is the latest book nominated for the Sunday times bestseller,the protaganist Elizabeth\\\n",
    "is a lab tehcnician at Royal Institue of Chemistry in the late 1960's when gender bias still prevailed.The story starts by\\\n",
    "telling how Elizabeth had achieved her masters degree despite the hurdles in life and goes on to face many more obstacles,that is \\\n",
    "when she encounters Dr.Alex,a well renowned scientist with several publications and accolades to his name,these two strangers\\\n",
    "end up on a journey of self-discovery and laughter.It's later towards the end we see how Elizabeth fights for women's rights\\\n",
    "and is forced to leaver her position as a chemist due to unforseen circumstances and ends up joining a television channel as \\\n",
    "a cook host who motivates several women all around the world to pursue their dreams by teaching them chemistry one molecule\\\n",
    "at a time\"\n",
    "\n",
    "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
    "def abstr(text,max_length=150):\n",
    "#abstractive summary generation\n",
    "    model_name = \"t5-base\"\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    input_ids=tokenizer.encode(\"summarize: \"+text,max_length=1024,return_tensors='pt',truncation=True)\n",
    "    print(input_ids)\n",
    "    summary_ids=model.generate(input_ids,max_length=max_length,num_beams=4,early_stopping=True,length_penalty=2.0)\n",
    "    print(summary_ids)\n",
    "    summary=tokenizer.decode(summary_ids[0],skip_special_tokens=True)\n",
    "    summary2=tokenizer.decode(summary_ids[0])\n",
    "    print(summary2)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "sumart=abstr(a1)\n",
    "print(sumart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db19b680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lessons of Chemistry is the latest book nominated for the Sunday times bestseller,the protaganist Elizabethis a lab tehcnician at Royal Institue of Chemistry in the late 1960's when gender bias still prevailed. The story starts bytelling how Elizabeth had achieved her masters degree despite the hurdles in life and goes on to face many more obstacles,that is when she encounters Dr.Alex,a well renowned scientist with several publications and accolades to his name,these two strangersend up on a journey of self-discovery and laughter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pooja\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\Pooja\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#extractive summary\n",
    "from summarizer import Summarizer\n",
    "summarizer=Summarizer()\n",
    "s=summarizer(a1)\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "311eea07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Barack/NNP)\n",
      "  (PERSON Obama/NNP)\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  in/IN\n",
      "  (GPE Hawaii/NNP)\n",
      "  and/CC\n",
      "  served/VBD\n",
      "  as/IN\n",
      "  the/DT\n",
      "  44th/CD\n",
      "  president/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION USA/NNP))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Pooja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Pooja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Pooja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Pooja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Program 9 -Perform Named Entity Recognition (NER)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "\n",
    "\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "text='Barack Obama was born in Hawaii and served as the 44th president of the USA'\n",
    "\n",
    "token=word_tokenize(text)\n",
    "pos=pos_tag(token)\n",
    "ner=ne_chunk(pos)\n",
    "print(ner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5888d6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Pooja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Pooja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original  the quick brown foxes jumped over the lazy dogs\n",
      "prt ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog']\n",
      "lem ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "#Program 10 - Perform Morphological Analysis\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "\n",
    "text='the quick brown foxes jumped over the lazy dogs'\n",
    "tokens=word_tokenize(text)\n",
    "ps=PorterStemmer()\n",
    "stem=[ps.stem(w) for w in tokens]\n",
    "    \n",
    "lemmatizer=WordNetLemmatizer()\n",
    "lem=[lemmatizer.lemmatize(w,pos=wordnet.VERB) for w in tokens]\n",
    "    \n",
    "print('original ',text)\n",
    "print('prt',stem)\n",
    "print('lem',lem)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fc183294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "understand\n",
      "['mis', 'ing', 'understand']\n"
     ]
    }
   ],
   "source": [
    "#morpheme\n",
    "word='misunderstanding'\n",
    "prefixes=['mis']\n",
    "root='understand'\n",
    "suffixes=['ing']\n",
    "d=[]\n",
    "for prefix in prefixes:\n",
    "    if word.startswith(prefix):\n",
    "        d.append(prefix)\n",
    "        word=word[len(prefix):]\n",
    "for suffix in suffixes:\n",
    "    if word.endswith(suffix):\n",
    "        d.append(suffix)\n",
    "        word=word[:-len(suffix)]\n",
    "        \n",
    "d.append(word)\n",
    "print(word)\n",
    "print(d)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7df24f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
